{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = Dataset.load_from_disk(\"unaugmented_data/train\")\n",
    "val = Dataset.load_from_disk(\"unaugmented_data/val\")\n",
    "dev = Dataset.load_from_disk(\"unaugmented_data/dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only want the text and label columns\n",
    "train_val_df = pd.concat([pd.DataFrame(train), pd.DataFrame(val)])[['text', 'pcl']]\n",
    "test_df = pd.DataFrame(dev)[['text', 'pcl']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data into bag of words representation\n",
    "vectorizer = CountVectorizer(token_pattern=\"[^\\W\\d_]+\")\n",
    "train_X = vectorizer.fit_transform(train_val_df.text.values)\n",
    "test_X = vectorizer.transform(test_df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run a logistic regression\n",
    "np.random.seed(100)\n",
    "logistic_reg = LogisticRegression()\n",
    "logistic_reg.fit(train_X, train_val_df.pcl.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(pred_y, true_y):\n",
    "    tp = sum((true_y == 1) & (pred_y == 1))\n",
    "    fp = sum((true_y == 0) & (pred_y == 1))\n",
    "    fn = sum((true_y == 1) & (pred_y == 0))\n",
    "    return tp / (tp + 0.5 * (fp + fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(pred_y, true_y):\n",
    "    tp = sum((true_y == 1) & (pred_y == 1))\n",
    "    fn = sum((true_y == 1) & (pred_y == 0))\n",
    "    return tp  / (tp + fn)\n",
    "\n",
    "\n",
    "def precision(pred_y, true_y):\n",
    "    tp = sum((true_y == 1) & (pred_y == 1))\n",
    "    fp = sum((true_y == 0) & (pred_y == 1))\n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_y, true_y):\n",
    "    return sum(pred_y==true_y)/len(true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of BoW Logistic Regression is 0.8948877209746775\n",
      "Precision of BoW Logistic Regression is 0.37349397590361444\n",
      "Recall of BoW Logistic Regression is 0.37349397590361444\n",
      "F1 Score of BoW Logistic Regression is 0.2198581560283688\n"
     ]
    }
   ],
   "source": [
    "# calculate F1 score and accuracy\n",
    "pred_y = logistic_reg.predict(test_X)\n",
    "print(f'Accuracy of BoW Logistic Regression is {accuracy(pred_y, test_df.pcl)}')\n",
    "print(f'Precision of BoW Logistic Regression is {precision(pred_y, test_df.pcl)}')\n",
    "print(f'Recall of BoW Logistic Regression is {precision(pred_y, test_df.pcl)}')\n",
    "print(f'F1 Score of BoW Logistic Regression is {f1_score(pred_y, test_df.pcl.values)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Misclassified Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_misclassified = test_df[test_df.pcl != pred_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct PCL = 1, PCL Score = 3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sheepherding in America has always been an immigrant 's job , too dirty , too cold and too lonely for anyone with options .\""
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_example1 = bow_misclassified.loc[16]\n",
    "print(f'Correct PCL = {bow_example1.pcl}, PCL Score = {pd.DataFrame(dev).labels.loc[16]}')\n",
    "bow_example1.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_iterator(tokenizer, series):\n",
    "    for text in series.values:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "def word2int(series, vocab, tokenizer):\n",
    "    return series.apply(lambda x: vocab(tokenizer(x)))\n",
    "\n",
    "def add_padding(ls):\n",
    "    padded_tensors = pad_sequence(ls, batch_first=True)\n",
    "    return padded_tensors.long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate vocabulary of the training data\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "vocab = build_vocab_from_iterator(text_iterator(tokenizer, train_val_df.text), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input text of training and testing data to integers from vocabulary\n",
    "train_tokens = word2int(train_val_df['text'], vocab, tokenizer)\n",
    "test_tokens = word2int(test_df['text'], vocab, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the tokens such that the input tensor is the same size\n",
    "all_tokens = [torch.Tensor(t) for t in train_tokens] + [torch.Tensor(t) for t in test_tokens]\n",
    "all_padded_tokens = add_padding(all_tokens)\n",
    "train_X = all_padded_tokens[:len(train_tokens)]\n",
    "test_X = all_padded_tokens[len(train_tokens):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pcl labels to tensor\n",
    "train_y = torch.LongTensor(train_val_df['pcl'].values)\n",
    "test_y = torch.LongTensor(test_df['pcl'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_f1_score(pred_y, true_y):\n",
    "    tp = torch.sum((true_y == 1) & (pred_y == 1)).item()\n",
    "    fp = torch.sum((true_y == 0) & (pred_y == 1)).item()\n",
    "    fn = torch.sum((true_y == 1) & (pred_y == 0)).item()\n",
    "    return tp / (tp + 0.5 * (fp + fn))\n",
    "\n",
    "def torch_recall(pred_y, true_y):\n",
    "    tp = torch.sum((true_y == 1) & (pred_y == 1)).item()\n",
    "    fn = torch.sum((true_y == 1) & (pred_y == 0)).item()\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def torch_precision(pred_y, true_y):\n",
    "    tp = torch.sum((true_y == 1) & (pred_y == 1)).item()\n",
    "    fp = torch.sum((true_y == 0) & (pred_y == 1)).item()\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def torch_accuracy(pred_y, true_y):\n",
    "    return torch.sum(pred_y == true_y) / len(true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_model(model, train_X, train_y, n_epochs, loss_fn, optimizer, batch_size):\n",
    "\n",
    "    n_batches = 1 + len(train_X) // batch_size\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        for i in range(n_batches):\n",
    "\n",
    "            batch_X = train_X[i*batch_size:(i+1)*batch_size].to(DEVICE)\n",
    "            batch_y = train_y[i*batch_size:(i+1)*batch_size].to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            torch.cuda.empty_cache()\n",
    "            pred_y = model(batch_X)\n",
    "            loss = loss_fn(pred_y.view(-1), batch_y.float()) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        model.eval()\n",
    "        print(f\"Epoch {epoch}: loss={loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralModel(nn.Module):\n",
    "    def __init__(self, vocab_size, num_tokens, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.fc1 = nn.Linear(embedding_dim*num_tokens, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_layer = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_layer(x).view(x.shape[0], -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.relu(self.fc3(x))\n",
    "        x = nn.functional.relu(self.fc4(x))\n",
    "        return self.out_layer(x)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        probs = nn.functional.sigmoid(self.forward(x))\n",
    "        return torch.where(probs >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for feedforward neural network with embeddings\n",
    "torch.manual_seed(100)\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "neural_model = NeuralModel(len(vocab), train_X.shape[1], 128, 128).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(neural_model.parameters(), lr=0.0001)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: F1=0.0\n",
      "Epoch 0: loss=0.4651470184326172\n",
      "Epoch 1: F1=0.00023850558528114147\n",
      "Epoch 1: loss=0.2583751678466797\n",
      "Epoch 2: F1=0.08733484750162232\n",
      "Epoch 2: loss=0.07993243634700775\n",
      "Epoch 3: F1=0.09260686259424526\n",
      "Epoch 3: loss=0.009527729824185371\n",
      "Epoch 4: F1=0.09390190813257299\n",
      "Epoch 4: loss=0.0023256358690559864\n",
      "Epoch 5: F1=0.09462652667263054\n",
      "Epoch 5: loss=0.0023603145964443684\n",
      "Epoch 6: F1=0.0947462310376285\n",
      "Epoch 6: loss=0.0007915312307886779\n",
      "Epoch 7: F1=0.09480597014925374\n",
      "Epoch 7: loss=0.00037500273901969194\n",
      "Epoch 8: F1=0.09480597014925374\n",
      "Epoch 8: loss=0.0002443444973323494\n",
      "Epoch 9: F1=0.09480597014925374\n",
      "Epoch 9: loss=0.0001699640997685492\n",
      "Epoch 10: F1=0.09480597014925374\n",
      "Epoch 10: loss=0.0001245349703822285\n",
      "Epoch 11: F1=0.09480597014925374\n",
      "Epoch 11: loss=9.506844071438536e-05\n",
      "Epoch 12: F1=0.09480597014925374\n",
      "Epoch 12: loss=7.464035297743976e-05\n",
      "Epoch 13: F1=0.09480597014925374\n",
      "Epoch 13: loss=5.981963477097452e-05\n",
      "Epoch 14: F1=0.09480597014925374\n",
      "Epoch 14: loss=4.8794983740663156e-05\n",
      "Epoch 15: F1=0.09480597014925374\n",
      "Epoch 15: loss=4.03714511776343e-05\n",
      "Epoch 16: F1=0.09480597014925374\n",
      "Epoch 16: loss=3.3789066947065294e-05\n",
      "Epoch 17: F1=0.09480597014925374\n",
      "Epoch 17: loss=2.8575763280969113e-05\n",
      "Epoch 18: F1=0.09480597014925374\n",
      "Epoch 18: loss=2.4359009330510162e-05\n",
      "Epoch 19: F1=0.09480597014925374\n",
      "Epoch 19: loss=2.090271664201282e-05\n",
      "Epoch 20: F1=0.09480597014925374\n",
      "Epoch 20: loss=1.80552524398081e-05\n",
      "Epoch 21: F1=0.09480597014925374\n",
      "Epoch 21: loss=1.5675806935178116e-05\n",
      "Epoch 22: F1=0.09480597014925374\n",
      "Epoch 22: loss=1.3690729247173294e-05\n",
      "Epoch 23: F1=0.09480597014925374\n",
      "Epoch 23: loss=1.1989513950538822e-05\n",
      "Epoch 24: F1=0.09480597014925374\n",
      "Epoch 24: loss=1.0550502338446677e-05\n",
      "Epoch 25: F1=0.09480597014925374\n",
      "Epoch 25: loss=9.313022019341588e-06\n",
      "Epoch 26: F1=0.09480597014925374\n",
      "Epoch 26: loss=8.244573109550402e-06\n",
      "Epoch 27: F1=0.09480597014925374\n",
      "Epoch 27: loss=7.329988875426352e-06\n",
      "Epoch 28: F1=0.09480597014925374\n",
      "Epoch 28: loss=6.528095127578126e-06\n",
      "Epoch 29: F1=0.09480597014925374\n",
      "Epoch 29: loss=5.825891548738582e-06\n"
     ]
    }
   ],
   "source": [
    "train_neural_model(neural_model, train_X.to(DEVICE), train_y.to(DEVICE), 30, loss_fn, optimizer, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Fully-Connected Neural Model is 0.8881987929344177\n",
      "F1 Score of Fully-Connected Neural Model is 0.1\n"
     ]
    }
   ],
   "source": [
    "neural_model.eval()\n",
    "pred_y = neural_model.predict(test_X.to(DEVICE)).flatten()\n",
    "print(f'Accuracy of Fully-Connected Neural Model is {torch_accuracy(pred_y.to(DEVICE), test_y.to(DEVICE))}')\n",
    "print(f'F1 Score of Fully-Connected Neural Model is {torch_f1_score(pred_y.to(DEVICE), test_y.to(DEVICE))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, num_tokens, embedding_dim, hidden_dim, n_layers):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, batch_first=True, bidirectional=True)\n",
    "        self.out_layer = nn.Linear(2*hidden_dim*num_tokens, 1)\n",
    "\n",
    "        self.prev_hid_states = None\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # initialise hidden state for RNN\n",
    "        if self.prev_hid_states is None:\n",
    "            self.prev_hid_states = torch.zeros(2*self.n_layers, x.shape[0], self.hidden_dim).to(DEVICE).contiguous()\n",
    "        torch.cuda.empty_cache()\n",
    "        x = self.embedding_layer(x.to(DEVICE))\n",
    "        x, final_hid_state = self.rnn(x, self.prev_hid_states[:,-x.shape[0]:].to(DEVICE).contiguous())\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.prev_histories = final_hid_state.detach()\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        torch.cuda.empty_cache()\n",
    "        return self.out_layer(x)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.prev_hid_states = None\n",
    "        torch.cuda.empty_cache()\n",
    "        probs = nn.functional.sigmoid(self.forward(x))\n",
    "        return torch.where(probs >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for RNN model\n",
    "torch.manual_seed(100)\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "rnn_model = RNNModel(len(vocab), train_X.shape[1], 64, 64, 5).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=0.0001)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.4061359167098999\n",
      "Epoch 1: loss=0.30582019686698914\n",
      "Epoch 2: loss=0.11218927800655365\n",
      "Epoch 3: loss=0.057111956179142\n",
      "Epoch 4: loss=0.03733159974217415\n",
      "Epoch 5: loss=0.023701535537838936\n",
      "Epoch 6: loss=0.018041936680674553\n",
      "Epoch 7: loss=0.010944971814751625\n",
      "Epoch 8: loss=0.009154314175248146\n",
      "Epoch 9: loss=0.003790356684476137\n",
      "Epoch 10: loss=0.003909831866621971\n",
      "Epoch 11: loss=0.0019716673996299505\n",
      "Epoch 12: loss=0.0020418993663042784\n",
      "Epoch 13: loss=0.00120650720782578\n",
      "Epoch 14: loss=0.0004596534126903862\n",
      "Epoch 15: loss=0.000764906988479197\n",
      "Epoch 16: loss=0.004254969768226147\n",
      "Epoch 17: loss=0.0065689594484865665\n",
      "Epoch 18: loss=0.0008361065993085504\n",
      "Epoch 19: loss=0.0006287939031608403\n",
      "Epoch 20: loss=0.0004637915117200464\n",
      "Epoch 21: loss=0.00031181698432192206\n",
      "Epoch 22: loss=0.00018825543520506471\n",
      "Epoch 23: loss=0.00010579312220215797\n",
      "Epoch 24: loss=5.7114451919915155e-05\n",
      "Epoch 25: loss=3.010783257195726e-05\n",
      "Epoch 26: loss=1.5377743693534285e-05\n",
      "Epoch 27: loss=8.021014764381107e-06\n",
      "Epoch 28: loss=4.359633749118075e-06\n",
      "Epoch 29: loss=2.384179651926388e-06\n"
     ]
    }
   ],
   "source": [
    "train_neural_model(rnn_model, train_X, train_y, 30, loss_fn, optimizer, batch_size=8)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bidirectional RNN Neural Model is 0.8738652467727661\n",
      "Recall of Bidirectional RNN Neural Model is 0.11557788944723618\n",
      "Precision of Bidirectional RNN Neural Model is 0.2072072072072072\n",
      "F1 Score of Bidirectional RNN Neural Model is 0.14838709677419354\n"
     ]
    }
   ],
   "source": [
    "rnn_model.eval()\n",
    "completed_n = 0\n",
    "batch_size = 8\n",
    "all_pred = torch.empty(len(test_X))\n",
    "while completed_n < len(test_X):\n",
    "    batch_X = test_X[completed_n: completed_n+batch_size].to(DEVICE)\n",
    "    all_pred[completed_n: completed_n+batch_size] = rnn_model.predict(batch_X).cpu().flatten()\n",
    "    torch.cuda.empty_cache()\n",
    "    completed_n += batch_size\n",
    "\n",
    "print(f'Accuracy of Bidirectional RNN Neural Model is {torch_accuracy(all_pred, test_y)}')\n",
    "print(f'Recall of Bidirectional RNN Neural Model is {torch_recall(all_pred, test_y)}')\n",
    "print(f'Precision of Bidirectional RNN Neural Model is {torch_precision(all_pred, test_y)}')\n",
    "print(f'F1 Score of Bidirectional RNN Neural Model is {torch_f1_score(all_pred, test_y)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Misclassified Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_misclassified = test_df[(all_pred != test_y).detach().numpy()]\n",
    "rnn_misclassified_0 = rnn_misclassified[rnn_misclassified.pcl == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "par_id                                                                8702\n",
       "art_id                                                           @@4933018\n",
       "keyword                                                           homeless\n",
       "country_code                                                            sg\n",
       "text                     On a single night in January this year , the n...\n",
       "labels                                                                 0.0\n",
       "pcl                                                                      0\n",
       "label_category_vector                                [0, 0, 0, 0, 0, 0, 0]\n",
       "input_ids                [101, 2006, 1037, 2309, 2305, 1999, 2254, 2023...\n",
       "attention_mask           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "Name: 459, dtype: object"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dev).loc[459]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On a single night in January this year , the number of homeless individuals in the US was 564,708 , according to The 2015 Annual Homeless Assessment Report released by The US Department of Housing and Urban Development last month .'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_misclassified_0.loc[459].text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
