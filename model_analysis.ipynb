{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertModel\n",
    "from torch.nn.functional import mse_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = Dataset.load_from_disk(\"data/dev\")\n",
    "dev_df = pd.DataFrame(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country_code</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>pcl</th>\n",
       "      <th>label_category_vector</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107</td>\n",
       "      <td>@@16900972</td>\n",
       "      <td>homeless</td>\n",
       "      <td>ke</td>\n",
       "      <td>His present \" chambers \" may be quite humble ,...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[101, 2010, 2556, 1000, 8477, 1000, 2089, 2022...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149</td>\n",
       "      <td>@@1387882</td>\n",
       "      <td>disabled</td>\n",
       "      <td>us</td>\n",
       "      <td>Krueger recently harnessed that creativity to ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[101, 1047, 6820, 26320, 3728, 17445, 2098, 20...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>151</td>\n",
       "      <td>@@19974860</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>in</td>\n",
       "      <td>10:41am - Parents of children who died must ge...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[101, 2184, 1024, 4601, 3286, 1011, 3008, 1997...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154</td>\n",
       "      <td>@@20663936</td>\n",
       "      <td>disabled</td>\n",
       "      <td>ng</td>\n",
       "      <td>When some people feel causing problem for some...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 1, 1, 1, 0]</td>\n",
       "      <td>[101, 2043, 2070, 2111, 2514, 4786, 3291, 2005...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157</td>\n",
       "      <td>@@21712008</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>ca</td>\n",
       "      <td>We are alarmed to learn of your recently circu...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 0, 0, 1, 1, 0]</td>\n",
       "      <td>[101, 2057, 2024, 19260, 2000, 4553, 1997, 211...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>10463</td>\n",
       "      <td>@@4676355</td>\n",
       "      <td>refugee</td>\n",
       "      <td>pk</td>\n",
       "      <td>\" The Pakistani police came to our house and t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[101, 1000, 1996, 9889, 2610, 2234, 2000, 2256...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>10464</td>\n",
       "      <td>@@19612634</td>\n",
       "      <td>disabled</td>\n",
       "      <td>ie</td>\n",
       "      <td>When Marie O'Donoghue went looking for a speci...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[101, 2043, 5032, 1051, 1005, 2123, 8649, 2016...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>10465</td>\n",
       "      <td>@@14297363</td>\n",
       "      <td>women</td>\n",
       "      <td>lk</td>\n",
       "      <td>Sri Lankan norms and culture inhibit women fro...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[101, 5185, 16159, 17606, 1998, 3226, 26402, 2...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>10466</td>\n",
       "      <td>@@70091353</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>ph</td>\n",
       "      <td>He added that the AFP will continue to bank on...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[101, 2002, 2794, 2008, 1996, 21358, 2361, 209...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>10468</td>\n",
       "      <td>@@16753236</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>in</td>\n",
       "      <td>\" Anja Ringgren Loven I ca n't find a word to ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 1, 0, 0, 1, 0]</td>\n",
       "      <td>[101, 1000, 2019, 3900, 3614, 13565, 2293, 207...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2093 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      par_id      art_id        keyword country_code  \\\n",
       "0        107  @@16900972       homeless           ke   \n",
       "1        149   @@1387882       disabled           us   \n",
       "2        151  @@19974860  poor-families           in   \n",
       "3        154  @@20663936       disabled           ng   \n",
       "4        157  @@21712008  poor-families           ca   \n",
       "...      ...         ...            ...          ...   \n",
       "2088   10463   @@4676355        refugee           pk   \n",
       "2089   10464  @@19612634       disabled           ie   \n",
       "2090   10465  @@14297363          women           lk   \n",
       "2091   10466  @@70091353     vulnerable           ph   \n",
       "2092   10468  @@16753236       hopeless           in   \n",
       "\n",
       "                                                   text  labels  pcl  \\\n",
       "0     His present \" chambers \" may be quite humble ,...     3.0    1   \n",
       "1     Krueger recently harnessed that creativity to ...     2.0    1   \n",
       "2     10:41am - Parents of children who died must ge...     3.0    1   \n",
       "3     When some people feel causing problem for some...     4.0    1   \n",
       "4     We are alarmed to learn of your recently circu...     4.0    1   \n",
       "...                                                 ...     ...  ...   \n",
       "2088  \" The Pakistani police came to our house and t...     0.0    0   \n",
       "2089  When Marie O'Donoghue went looking for a speci...     0.0    0   \n",
       "2090  Sri Lankan norms and culture inhibit women fro...     1.0    0   \n",
       "2091  He added that the AFP will continue to bank on...     0.0    0   \n",
       "2092  \" Anja Ringgren Loven I ca n't find a word to ...     4.0    1   \n",
       "\n",
       "      label_category_vector  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 1]   \n",
       "1     [1, 0, 0, 0, 0, 0, 1]   \n",
       "2     [1, 0, 0, 1, 0, 0, 0]   \n",
       "3     [0, 0, 1, 1, 1, 1, 0]   \n",
       "4     [1, 1, 0, 0, 1, 1, 0]   \n",
       "...                     ...   \n",
       "2088  [0, 0, 0, 0, 0, 0, 0]   \n",
       "2089  [0, 0, 0, 0, 0, 0, 0]   \n",
       "2090  [0, 0, 0, 0, 0, 0, 0]   \n",
       "2091  [0, 0, 0, 0, 0, 0, 0]   \n",
       "2092  [1, 0, 1, 0, 0, 1, 0]   \n",
       "\n",
       "                                              input_ids  \\\n",
       "0     [101, 2010, 2556, 1000, 8477, 1000, 2089, 2022...   \n",
       "1     [101, 1047, 6820, 26320, 3728, 17445, 2098, 20...   \n",
       "2     [101, 2184, 1024, 4601, 3286, 1011, 3008, 1997...   \n",
       "3     [101, 2043, 2070, 2111, 2514, 4786, 3291, 2005...   \n",
       "4     [101, 2057, 2024, 19260, 2000, 4553, 1997, 211...   \n",
       "...                                                 ...   \n",
       "2088  [101, 1000, 1996, 9889, 2610, 2234, 2000, 2256...   \n",
       "2089  [101, 2043, 5032, 1051, 1005, 2123, 8649, 2016...   \n",
       "2090  [101, 5185, 16159, 17606, 1998, 3226, 26402, 2...   \n",
       "2091  [101, 2002, 2794, 2008, 1996, 21358, 2361, 209...   \n",
       "2092  [101, 1000, 2019, 3900, 3614, 13565, 2293, 207...   \n",
       "\n",
       "                                         attention_mask  \n",
       "0     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "...                                                 ...  \n",
       "2088  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2089  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2090  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2091  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2092  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "\n",
       "[2093 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Model module here ##\n",
    "class CustomBert(nn.Module):\n",
    "    def __init__(self, transformer_out=6, dropout=0.1, class_weights=None):\n",
    "        super(CustomBert, self).__init__()\n",
    "        # Instead of just using the output of the final hidden layer,\n",
    "        # you can also pass in a range of hidden layers to concatenate their outputs\n",
    "        self.transformer_out = (\n",
    "            range(transformer_out, transformer_out + 1)\n",
    "            if isinstance(transformer_out, int)\n",
    "            else transformer_out\n",
    "        )\n",
    "        out_dim = len(self.transformer_out) * 768\n",
    "\n",
    "        # Use pretrained DistilBert. Force it to use our dropout\n",
    "        self.distilbert = DistilBertModel.from_pretrained(\n",
    "            \"distilbert-base-uncased\", output_hidden_states=True\n",
    "        )  # type: DistilBertModel\n",
    "        for module in self.distilbert.modules():\n",
    "            if isinstance(module, torch.nn.Dropout):\n",
    "                module.p = dropout\n",
    "\n",
    "        # Then apply a dense hidden layer down to 768, and a final layer down to 1\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(out_dim, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(768, 1),\n",
    "        )\n",
    "\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = class_weights\n",
    "            self.pos_weight = class_weights[1] / class_weights[0]\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Recommended pooling approach for DistilBert is to average over the hidden state sequence\n",
    "        # instead of outputs.last_hidden_state[:, 0], which is used for Bert which uses [CLS] token\n",
    "        pooled_output = []\n",
    "        for i in self.transformer_out:\n",
    "            hs = outputs.hidden_states[i]\n",
    "            mask = attention_mask.unsqueeze(-1)\n",
    "            hs = hs * mask\n",
    "            mean_hs = hs.sum(dim=1) / mask.sum(dim=1)\n",
    "            pooled_output.append(mean_hs)\n",
    "\n",
    "        # We also concatenate the outputs of multiple layers if chosen by the user\n",
    "        cat_output = torch.cat(pooled_output, dim=1)\n",
    "\n",
    "        # Apply dense feedforward\n",
    "        y = self.feedforward(cat_output).squeeze(-1)\n",
    "\n",
    "        # Outside the Trainer, we return the predictions\n",
    "        if labels is None:\n",
    "            return y\n",
    "\n",
    "        # Inside the Trainer, we also need to return the loss\n",
    "        global binary_classifier\n",
    "        if binary_classifier:\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                y, labels, pos_weight=self.pos_weight\n",
    "            ).to(DEVICE)\n",
    "        else:\n",
    "            loss = mse_loss(y, labels, reduction=\"none\").to(DEVICE)\n",
    "            weights = self.class_weights[labels.long().to(DEVICE)]\n",
    "            loss = loss * weights\n",
    "            loss = loss.mean()\n",
    "        return loss, y\n",
    "\n",
    "    def freeze(self):\n",
    "        for param in self.distilbert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze(self, layer=None):\n",
    "        for name, param in self.distilbert.named_parameters():\n",
    "            if layer is None or name.startswith(f\"transformer.layer.{layer}\"):\n",
    "                param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomBert(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feedforward): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomBert() # instantiate model\n",
    "\n",
    "# load in trained parameters\n",
    "checkpoint_fp = 'results/model.pth'\n",
    "checkpoint = torch.load(checkpoint_fp)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0970, 1.8491, 2.2538, 2.6964, 2.0818], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "atten_masks = torch.LongTensor([ls for ls in dev_df.attention_mask.values[:batch_size]])\n",
    "input_ids = torch.LongTensor([ls for ls in dev_df.input_ids.values[:batch_size]])\n",
    "model(input_ids, atten_masks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
